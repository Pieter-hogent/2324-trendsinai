{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LangChain\n",
    "\n",
    "(deels geïnspireerd door een lessenreeks van DeepLearning.ai)\n",
    "\n",
    "Voor we starten gaan we eerst zorgen dat iedereen een werkende setup heeft. Als je vorige keer OpenAI opgezet hebt, en je api key staat nog altijd mooi in de `.env` dan is alles oké.\n",
    "Als je credits op waren, is er een alternatief via [Eden AI](https://www.edenai.co/), Eden AI heeft als business model om via één centrale API (de hunne) allerlei andere AI API's aan te spreken. Maar wat voor ons vooral handig is, is dat je een account kan aanmaken zonder credit card, en dan $10 gratis credits krijgt (en dus kunnen we via hen OpenAI aanspreken)\n",
    "\n",
    "Ga naar [www.edenai.co](https://www.edenai.co/) en maak een account aan.\n",
    "\n",
    "<div>\n",
    "    <img src=\"img/edenai_signup.png\" style=\"width: 700px;\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "Dan kan je bij Account Management je API key vinden\n",
    "\n",
    "<div>\n",
    "    <img src=\"img/edenai_apikey.png\" style=\"width: 700px;\" >\n",
    "</div>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3047d615ba7724e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bewaar nu deze API key ook in je `.env` file (de .env file moet ergens in een parent folder van al je notebooks staan, dus als je de repo gewoon gecloned hebt gewoon in de `2324-trendsinai/` folder zelf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "991dc3aff46d6eb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "OPENAI_API_KEY=sk-YYYYY\n",
    "EDENAI_API_KEY=XXXXXXXX\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9698aa9a6daa2d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "edenai_api_key = os.getenv('EDENAI_API_KEY')\n",
    "\n",
    "# print (openai.api_key)\n",
    "# print (edenai_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als je de print commando's uitvoert, zou je de key(s) die je wilt gebruiken moeten zien, als je `None` ziet staan is er iets mis.\n",
    "\n",
    "De key van EdenAI is een Bearer token, daar kennen jullie alles van uit de Webservices cursus. Rechtstreeks de EdenAI API aanspreken doe je met een POST request naar de juiste url.\n",
    "EdenAI heeft véél endpoints, een ChatGPT request zou er als volgt uitzien:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88c88e58a348b026"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.edenai.run/v2/text/chat\"\n",
    "\n",
    "payload = {\n",
    "    \"response_as_dict\": True,\n",
    "    \"attributes_as_list\": False,\n",
    "    \"show_original_response\": False,\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"providers\": \"openai\",\n",
    "    \"text\": \"what are the advantages of langchain?\"\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": f\"Bearer {edenai_api_key}\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b69b7a377cf9bb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We krijgen een string waar ogenschijnlijk wel een JSON-object in zit met key `openai` (we kunnen dit ook direct naar meerdere LLM's sturen en dan zien we elke response), een `status` (hopelijk success) en dan de `generated_text`, er volgt ook nog een array met de role-user, role-assistant historiek in het geval van openai, en op het einde een `cost`.\n",
    "\n",
    "Toen ik deze vraag stelde, kostte het antwoord mij 0.000708; dus met onze 10 dollar kunnen we een 15.000 van dat soort requests sturen, veel, maar ook niet oneindig.\n",
    "\n",
    "We moeten dit JSON-object wel eerst omzetten naar Python, voor we verder kunnen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "179b32c69405199f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "data = '{\"openai\":{\"status\":\"success\",\"generated_text\":\"Example of generated_text\" }}'\n",
    "\n",
    "# Parse JSON into an object with attributes corresponding to dict keys.\n",
    "x = json.loads(data, object_hook=lambda d: SimpleNamespace(**d))\n",
    "print(x.openai.generated_text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6096caf49f71158"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als we nu alles samenbrengen kunnen we onze `get_answer` van vorige keer aanpassen om of OpenAI, of EdenAI of GPT4All te gebruiken al naargelang."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "172874dd65896cc0"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c13e3d737038ed61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"ggml-model-gpt4all-falcon-q4_0.bin\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5177dda8b54bd53a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class API(Enum):\n",
    "    OPEN_AI = 1\n",
    "    GPT4All = 2\n",
    "    EDEN_AI = 3\n",
    "\n",
    "\n",
    "def get_answer(prompt, which_model=API.OPEN_AI):\n",
    "    if which_model == API.GPT4All:\n",
    "        res = model.generate(prompt)\n",
    "        return res\n",
    "    elif which_model == API.OPEN_AI:\n",
    "        message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        res = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=message\n",
    "        )\n",
    "        return res.choices[0].message[\"content\"]\n",
    "    elif which_model == API.EDEN_AI:\n",
    "        url = \"https://api.edenai.run/v2/text/chat\"\n",
    "\n",
    "        payload = {\n",
    "            \"response_as_dict\": True,\n",
    "            \"attributes_as_list\": False,\n",
    "            \"show_original_response\": False,\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 1000,\n",
    "            \"providers\": \"openai\",\n",
    "            \"text\": f\"{prompt}\"\n",
    "        }\n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"authorization\": f\"Bearer {edenai_api_key}\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        responseObject = json.loads(response.text, object_hook=lambda d: SimpleNamespace(**d))\n",
    "        return responseObject.openai.generated_text;\n",
    "\n",
    "            "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e4f54b1eff41f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"GPT4All\")\n",
    "print(get_answer(\"What are the advantages of GPT4All\", API.GPT4All))\n",
    "print(\"Open AI\")\n",
    "print(get_answer(\"What are the advantages of OpenAI\", API.OPEN_AI))\n",
    "print(\"Eden AI\")\n",
    "print(get_answer(\"What are the advantages of EdenAI\", API.EDEN_AI))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "803dff20cc541e9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LangChain, waarom?\n",
    "\n",
    "LangChain is een framework dat dient om applicaties die gebruik maken van language models te ontwikkelen. Het laat toe om vrij eenvoudig verschillende componenten aan elkaar te 'chainen' (vandaar de naam), en zo meer gestandaardiseerd allerlei verschillende modellen te gebruiken.\n",
    "\n",
    "Het is zeer snel, zeer groot geworden (al meer dan 1700 contributors op [github](https://github.com/langchain-ai/langchain) voor de python versie, en nog eens 400 voor de js versie)\n",
    "Het leuke is dat je snel veel soorten AI modellen kan gebruiken, of makkelijk uitbreiden met vectorstores en andere extra functionaliteit; en je bij een wissel naar een ander model niet per se alles opnieuw dient te programmeren of aan te passen.\n",
    "\n",
    "Een (voorlopig?) nadeel is dat het zo snel evolueert en groeit dat de documentatie best wel te wensen over laat.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc2dfa7c6a2e8688"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models\n",
    "\n",
    "We gaan eerst een llm model creëeren, je hebt de keuze tussen een EdenAI of OpenAI model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6bbb9a5f5b73032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7f9bdc0f515ebf93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3afea169b09d5047"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompts\n",
    "\n",
    "Om makkelijker prompts te genereren die (deels) variabel zijn heeft LangChain [PromptTemplates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/), een PromptTemplate krijgt een string met {variabelen} als `template`, en genereert dan samen met een array van `input_variables` een prompt die kan gebruikt worden voor een llm.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6809c4300e7132"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ae02c5037c260e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "288d4ed488be6b3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output formatting\n",
    "\n",
    "De standaard output is gewoon een stuk tekst. Vaak wil je echter JSON of een ander gestructureerd formaat om dan makkelijker verder te kunnen werken (en 'chainen')\n",
    "\n",
    "Als voorbeeld nemen we een review van Disneyland, genomen uit een dataset van [https://www.kaggle.com/](https://www.kaggle.com/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9f7542d7767ff18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "review = f\"\"\"If you've ever been to Disneyland anywhere you'll find Disneyland Hong Kong very similar in the layout when you walk into main street! It has a very familiar feel. One of the rides  its a Small World  is absolutely fabulous and worth doing. The day we visited was fairly hot and relatively busy but the queues moved fairly well.\"\"\"\n",
    "\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "location: Was the review about Disneyland Paris, California, Hong Kong or Unknown\n",
    "\n",
    "weather: Was there any indication about the weather conditions, answer with \\\n",
    "'TOO HOT', 'HOT', 'RAIN', 'COLD' or 'UNKNOWN' if no information was provided\n",
    "\n",
    "rides: Extract any sentences about the rides,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "location\n",
    "weather\n",
    "rides\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d63045ab7a9aef69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b50b924d3a77a88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zoals je kan zien krijgen we een `string` terug van het systeem, het zou natuurlijk veel beter zijn als dit een echt JSON object is. We gebruiken hiervoor [StructuredOutputParser](https://python.langchain.com/docs/modules/model_io/output_parsers/structured)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3803182d9a6ae66a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e82c282b17df88bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indexes\n",
    "\n",
    "### Conversation Memory\n",
    "\n",
    "Met behulp van een [https://python.langchain.com/docs/modules/memory/types/buffer](Conversation Buffer) krijgt je conversatie een 'geheugen', je hoeft niet langer de prompts in hun geheel mee te geven. (standaard start elke prompt van nul, en kan de AI dus geen rekening houden met wat er reeds eerder gezegd is)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ad605ea1202c88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6621ea0e36a28448"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4891ed6a77d86312"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5ee01a3001da4a9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "44b4b92f6c043d78"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c0bc7dd7873cd70e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Je kan de grootte van je conversation buffer zelf beheren."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fee5425f3c2fb00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d14f4c2c8ded73ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In plaats van de buffer te beperken op basis van het aantal vraag-antwoorden, kan je het ook op basis van het aantal tokens doen met behulp van `ConversationTokenBufferMemory`\n",
    "\n",
    "### oefening \n",
    "\n",
    "Gebruik TokenBufferMemory om een deel van een conversatie te beperken, kijk wat er gebeurt als de 'helft' van een zin wegvalt door de limit\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5be15fdecdabe1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Een `ConversationSummaryBufferMemory` gaat niet gewoon alles uit het geheugen gooien als er geen plaats meer is (en alles waarvoor er wel plaats is letterlijk onthouden) maar gaat proberen een samenvatting te maken van wat anders zou verdwijnen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a60a694eeb81c643"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9644b0aca8e824f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chains\n",
    "\n",
    "Lang*Chain*, er bestaan een heleboel 'Chain' klassen die je toestaan de output van één prompt als input voor de volgende te gebruiken. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1fab74d59589177"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "367b9199ef6c832d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bij een `SimpleSequentialChain` wordt de output van de ene als input voor de volgende gebruikt, zonder dat er nood is aan een key of iets dergelijks. Het systeem gaat er vanuit dat de output van de ene de (enige) input van de andere is."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89153558ae8f458"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "38968412ffc2191d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als er iets complexere kettingen dienen gevormd te worden, waar een prompt input dient te krijgen van twee verschillende andere prompts bijvoorbeeld, gebruiken we een `SequentialChain`, hier kan je telkens een key associëren met een output, en zo meer controle krijgen over welke output bij welke input gebruikt wordt. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87301d723e76153b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "af3d39ff8844336e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RetrievalQA: Q&A over documenten\n",
    "\n",
    "We hebben al heel wat klassen van langchain de revue zien passeren en kleinere chains gebouwd. Tijd om eens een wat 'echtere' toepassingen na te bootsen.\n",
    "Nu gaan we een grote(re) hoeveelheid data inladen gebruik makend van een vectorstore; en dan een Q&A systeem over deze data creëeren.\n",
    "\n",
    "Als voorbeeld nemen we een dataset van Steam games, bekijk eerst de data even in een terminal (of iets dergelijks). \n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5737fef699ff1912"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d268d8a41be321a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Embeddings` zorgen ervoor dat we een vector representatie van een stuk tekst kunnen genereren., "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f01b4c8125ad64c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8309640d6cf67499"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1e1d71104c8bc50d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We kunnen nu similarity searches uitvoeren gebruik makend van vectorstores. Een makkelijk te hanteren versie is de `DocArrayInMemorySearch`. Zoals de naam suggereert gebeurt alles in memory, dus enkel geschikt als de data niet té groot is, maar heeft het grote voordeel dat we geen externe vector store API's moeten aanspreken of configureren."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2333bafe66a84660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "57ae98639280ca00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zo wordt er wel een gelijkaardig resultaat gevonden, op basis van de vector embedding, maar om wat 'menselijkere' output te krijgen, in de vorm van vraag-antwoord, gebruiken we de `RetrievalQA`\n",
    "\n",
    "Die zal een llm combineren met onze vectordatabank."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac5fc0716e64c38b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "deb70206b5b6e05d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Er bestaat ook een makkelijkere manier om hetzelfde te bekomen, een `VectorStoreIndexCreator`, die veel korter juist hetzelfde doet.\n",
    "Alles nog eens opnieuw (dus ook document loaden e.d., om te tonen hoe weinig code nodig is)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3c348759a3fa342"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2847a1ef84b87595"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9c09267aab40005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b1fee69cd1b8d865"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evalueren\n",
    "\n",
    "Hoe kunnen we nu zien hoe goed deze chains hun werk doen? We zouden manueel kunnen door onze data gaan en goede voorbeelden vinden, daar vragen op loslaten, en kijken of de antwoorden voldoen.\n",
    "Maar dat schaalt natuurlijk absoluut niet. Dus we gaan een llm chain gebruiken om onze llm chain te evalueren.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "965d06e4438f86b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "44813f2894326f7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dan kunnen we deze vragen uitvoeren en zien of de antwoorden correct gegeven worden, maar manueel schaalt dat natuurlijk weer niet zo goed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2721bce66910665"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad80ca3a6606ee26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7676b60b06208718"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dus we gaan al deze vragen door QAChain sturen "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e2d3f6e75c72fd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc17113618118e37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En dan kunnen we ze allemaal tegelijk laten evalueren met behulp van een QAEvalChain\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b13c7a2fab3563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca5c1b9415897ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agents\n",
    "\n",
    "Agents zijn een vrij nieuwe toevoeging aan LangChain, en laten toe om andere soorten externe bronnen toe te voegen. Bijvoorbeeld een rekenmodule, wikipedia of een zoekmachine."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5ffe0b3c8eeef6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae38fa177dafb118"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "545f27c0de0f4d1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

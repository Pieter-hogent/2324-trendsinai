{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "edenai_api_key = os.getenv('EDENAI_API_KEY')\n",
    "\n",
    "# print (openai.api_key)\n",
    "# print (edenai_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als je de print commando's uitvoert, zou je de key(s) die je wilt gebruiken moeten zien, als je `None` ziet staan is er iets mis.\n",
    "\n",
    "De key van EdenAI is een Bearer token, daar kennen jullie alles van uit de Webservices cursus. Rechtstreeks de EdenAI API aanspreken doe je met een POST request naar de juiste url.\n",
    "EdenAI heeft véél endpoints, een ChatGPT request zou er als volgt uitzien:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88c88e58a348b026"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c13e3d737038ed61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from langchain.llms import EdenAI, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.embeddings.edenai import EdenAiEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "class API(Enum):\n",
    "    OPEN_AI = 1\n",
    "    GPT4All = 2\n",
    "    EDEN_AI = 3\n",
    "    \n",
    "\n",
    "def get_llm(which_model=API.OPEN_AI, temperature = 0.0):\n",
    "    if which_model == API.OPEN_AI:\n",
    "        # OpenAI heeft ook een ChatModel, maar dat is niet makkelijk transparent te gebruiken als je ook EdenAI wil gebruiken in dezelfde requests. Als je enkel met OpenAI wenst te werken is dit zeker een goede optie. Je zal dan wel het output formaat moeten aanpassen.\n",
    "        # return ChatOpenAI(temperature=temperature, model=\"gpt-3.5-turbo\")\n",
    "        return OpenAI(temperature=temperature)\n",
    "    elif which_model == API.EDEN_AI:\n",
    "        return EdenAI(edenai_api_key=edenai_api_key,provider=\"openai\", model=\"gpt-3.5-turbo-instruct\", temperature=temperature, max_tokens=250)\n",
    "\n",
    "def get_embedding(which_model=API.OPEN_AI):\n",
    "    if which_model == API.OPEN_AI:\n",
    "        return OpenAIEmbeddings()\n",
    "    elif which_model == API.EDEN_AI:\n",
    "        return EdenAiEmbeddings(edenai_api_key=edenai_api_key,provider=\"openai\")\n",
    "       "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f9bdc0f515ebf93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preferred_model = API.OPEN_AI\n",
    "\n",
    "# llm = get_llm(API.EDEN_AI, 0.0)\n",
    "# creative_llm = get_llm(API.EDEN_AI, 0.9)\n",
    "llm = get_llm(preferred_model, 0.0)\n",
    "creative_llm = get_llm(preferred_model, 0.9)\n",
    "embeddings = get_embedding(preferred_model)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3afea169b09d5047"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chat with your data\n",
    "\n",
    "![langchain](img/langchain.jpg)\n",
    "\n",
    "(image credit: langchain.com)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13dd0747ea16853"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Document Loading\n",
    "\n",
    "Extra data kan in allerlei formaten voorkomen, PDF, JSON, tekst, ... en kan zowel gestructureerd of ongestructureerd zijn.\n",
    "\n",
    "Om dit te illustreren starten we met een PDF van \"The Little Book on Deep Learning\" (https://fleuret.org/public/lbdl.pdf), vooral omdat deze een creative commons license heeft."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cd4052ba1bc7bec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "547e97a9e03d1c56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7233034c74c88100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Youtube\n",
    "\n",
    "Maar je kan ook youtube video's als input nemen, we combineren de audio loader van youtube, and de OpenAI whisper parser, die samen een youtube video omzetten in een stuk tekst dat als bron kan dienen.\n",
    "(om dit te doen werken heb je ook ffmpeg nodig, MacOS/Linux: `brew/apt-get/... install ffmpeg`, Windows: downloaden en prutsen)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdcea0c78ceb57da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# yt_dlp heeft ffmpeg nodig; dus we zorgen dat het in het standaard path gevonden wordt\n",
    "# onderstaande paden zijn typisch voor MacOS homebrew, je zal ze waarschijnlijk moeten aanpassen naargelang je OS en waar je ffmpeg geïnstalleerd hebt\n",
    "import sys\n",
    "sys.path.append('/opt/homebrew/bin/ffmpeg')\n",
    "sys.path.append('/opt/homebrew/bin/ffprobe')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9acc799fcb9f167b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b5d313a207f81ba4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# from yt_dlp.postprocessor import FFmpegPostProcessor\n",
    "# FFmpegPostProcessor._ffmpeg_location.set('/opt/homebrew/bin/')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27ded5106e6dc3f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OPGELET\n",
    "#\n",
    "# dit deel uitvoeren duurt snel een aantal minuten; audio transcriben is ook relatief duur $0.006/min of dus $0.36 per uur\n",
    "# geen extreme bedragen, maar voer dit stuk code ook niet te vaak uit\n",
    "\n",
    "# Transformer Neural Networks van CodeEmporium\n",
    "url=\"https://www.youtube.com/watch?v=TQQlZhbC5ps\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72f0d507e6ad1683"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### URL\n",
    "\n",
    "Je kan ook gewoon alle content van een url inladen, om dan met de data van een website te 'chatten'. De standaard `WebBaseLoader` kan HTML inlezen en beschikbaar maken, dit werkt wel enkel voor sites die niet JavaScript 'heavy' zijn. Om sites die dynamisch opgebouwd worden te kunnen laden heb je een headless browser nodig. Selenium, te gebruiken via `SeleniumURLLoader` is een optie. (hoewel die dingen niet altijd even perfect werken)   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3168eea598d454d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc34b27323ab7770"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Document Splitting\n",
    "\n",
    "We willen de tekst in stukken splitsen, maar dat is veel subtieler dan je zou denken. Je wilt de tekst semantisch splitsen, de stukken zullen later een vector encodering krijgen en gebruikt worden om vragen te beantwoorden. Simpel gezegd wil je bijvoorbeeld niet dat een zin half in één stuk zit, en half in een ander.\n",
    "\n",
    "Er zijn een aantal splitters voorzien in [langchain](https://python.langchain.com/docs/modules/data_connection/document_transformers/#text-splitters): `CharacterTextSplitter`, `MarkdownHeaderTextSplitter`, `TokenTextSplitter`, `RecursiveCharacterTextSplitter` enz."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c85ac8f04fb21674"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a18302153960b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dfa420a19b48476b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4442fd348ca5c8f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "De `CharacterTextSplitter` doet ogenschijnlijk niets, dit komt omdat er gesplitst wordt op basis van een specifiek character (standaard een newline), en dus zal een text zonder newlines niet gesplitst worden \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab9e9d65be61ff2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "497b4796b7ce716e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In de praktijk is deze splitter dus niet zo handig, je wilt liefst zo semantisch mogelijk splitsen (per paragraaf of zin) en pas als die paragrafen / zinnen te lang worden, op basis van een ander character.\n",
    "\n",
    "Daartoe dient de `RecursiveCharacterTextSplitter`, i.p.v. één separator character te gebruiken, geef je een lijst van separators mee, er zal eerst gesplitst worden volgens het eerste character, als de stukken dan nog te lang zijn opnieuw via de tweede, enz.\n",
    "Tot er geen separators meer over zijn of elk stuk klein genoeg geworden is "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e8627e35b803cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8499b52ced99819c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1fc4c02af4e2fb99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d15acaed21ba04f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3584f1b8943285c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a24a92f23eb2bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1257b6054241ef53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aa3d0593866d097d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Een andere manier is om te splitsen op basis van tokens. Dit is belangrijk als we met LLM's werken, die een context hebben die beperkt is tot een aantal tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bafc82c6a4e4272"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3b15e6fe6b211d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3da0ddb7e0d9f2ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Voor markdown files gebruiken we best de `MarkdownHeaderTextSplitter`, zoals de naam suggereert worden markdown files gesplitst op basis van de headers, en de informatie uit die headers komt dan in de metadata terecht."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f689ad468cf2be6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "94c5b963e3ae0668"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### vectorstores\n",
    "\n",
    "De volgende stap is al deze chunks in een vectorstore op te slaan, zodat we snel en makkelijk 'gelijkaardige' content kunnen terugvinden (die we dan samen met onze vraag naar een LLM doorsturen, maar dat komt later)\n",
    "\n",
    "We dienen eerste vector embeddings te creëren, vector voorstellingen van onze stukken tekst, hiervoor gebruiken we OpenAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc24accd63123f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b17c9f5f87141dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chroma\n",
    "\n",
    "Chroma is een vectorstore die volledig in het geheugen draait, ideaal om snel wat code te runnen (en te demonstreren hier), voor grotere toepassingen bestaan er veel gehoste oplossing ook, langchain heeft bindings voor de meest courant gebruikte."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ad414706c2d9631"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4c2fe9a6413462c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "143c4a4fe387cf62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d7fba208205c326e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6a025626d6654dde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fbd6ad8ed2cd6585"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aa5cbf64bafc9e75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieval\n",
    "\n",
    "Maximum marginal relevance (MMR), als je gewoon de meest gelijkende resultaten zoekt, gebeurt het soms dat je een aantal resultaten terug krijgt die redundant zijn; die eigenlijk allemaal dezelfde inhoud terug geven. MMR kan hier helpen, het algoritme zal naast de relevantie van de resultaten, ook de 'diversiteit' in rekening nemen, en op basis van beide een nieuwe ranking maken. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be670f4a648d67fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a04692982d638fe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4fdf44cd11ea0618"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c8630bf9bf8ff46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "SelfQuery, een ander algoritme dat je met langchain kan gebruiken is SelfQuery, het idee is dat je je vraag stelt in 'natuurlijke taal', en dat de LLM zichzelf gebruikt (vandaar de naam) om onderscheid te maken tussen delen van de vraag waarmee de metadata kan gefilterd worden, en de eigenlijk inhoud zelf. \n",
    "\n",
    "Als we kijken welke metadata er in het resultaat van onze similarity_search zit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0226dd4e75234ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "37595c357758a6ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "60f8452e23dd90d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "475c7f249e7a4f81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c66c5d751cf57a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4b2f461eae61ce8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "600232b512f98eef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vragen beantwoorden\n",
    "\n",
    "We hebben gezien hoe we data kunnen inladen, hoe we documenten kunnen splitten en in een vectorstore krijgen, en hoe we dan een relevant document kunnen opvragen voor onze vragen.\n",
    "\n",
    "De volgende stap is nu de LLM deze vraag te laten beantwoorden, met behulp van dit document"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be4f0e0a742392fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a421318d7dc4266"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b4a6fccc07b786"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e1dddc9e2a4d9ce8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "19acf46da1ac7ad1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d56feab5ddee85f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "832c45f65e1e47e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### map reduce\n",
    "\n",
    "Als de documenten te uitgebreid zijn, zullen ze al snel groter zijn dan de beschikbare context voor LLM's. Een oplossing is om map reduce toe te passen, simpel gezegd zal je de documenten opsplitsen, de vraag naar elk sturen 'mappen', en dan de verschillende antwoorden 'reducen'.\n",
    "\n",
    "Dit leidt snel tot vrij veel API calls dus we gaan dit hier niet demonstreren. Er zijn voorbeelden en uitleg te vinden op langchain als je dit nodig hebt.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df0c1ceb0525d314"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat\n",
    "\n",
    "Om nu echt te kunnen chatten met de data ontbreekt er nog een deel van de puzzel, we moeten de eerder gegeven antwoorden ook kunnen meenemen in een volgende vraag. Zodat we extra verduidelijking kunnen krijgen, zoals we dat tegenwoordig gewend zijn van chatbots.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f55b2ec3adccfe60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6821fe5e7d5f7823"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f02490e0875f5e97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "435eca598799b5ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
